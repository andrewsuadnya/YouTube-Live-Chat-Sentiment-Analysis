# ðŸ“± YouTube Live Chat Sentiment Analysis (Real-Time Data Pipeline)

A real-time system for ingesting, processing, and analyzing YouTube Live Chat comments using modern Big Data technologies. Built using **Apache Kafka**, **Spark Structured Streaming**, **Elasticsearch**, **Kibana**, **React.js**, and **Flask**, this pipeline performs sentiment analysis using **VADER** and **TextBlob**, and visualizes results on an interactive dashboard.

---

## ðŸ”§ Tech Stack
![Data Pipeline2](https://github.com/user-attachments/assets/384d4341-ddf0-4a7a-bdf2-6448aa926a3d)
The data pipeline consists of five main stages:
â‘  YouTube Live Chat is retrieved using the YouTube Data API v3 by a custom Kafka producer.
â‘¡ The messages are ingested into Apache Kafka, acting as a distributed messaging system.
â‘¢ Apache Spark Structured Streaming consumes data from Kafka, performs cleaning, transformation, and sentiment analysis (VADER & TextBlob).
â‘£ The enriched data is sent to Elasticsearch for storage and indexing.
â‘¤ Data is then visualized in real-time via Kibana and a custom React.js + Flask dashboard using WebSockets.

* **[YouTube Data API v3](https://developers.google.com/youtube/v3)** â€“ Retrieves live chat messages from YouTube (data source)  
* **[Apache Kafka](https://kafka.apache.org/)** â€“ Message broker for real-time data ingestion  
* **[Apache Spark Structured Streaming](https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html)** â€“ Stream processing engine for data cleaning, transformation, and sentiment analysis  
* **[VADER](https://github.com/cjhutto/vaderSentiment)** & **[TextBlob](https://textblob.readthedocs.io/)** â€“ Lexicon-based sentiment analysis  
* **[Elasticsearch](https://www.elastic.co/elasticsearch/)** â€“ Searchable data store for storing analyzed data  
* **[Kibana](https://www.elastic.co/kibana/)** â€“ Real-time analytics visualization tool for querying Elasticsearch  
* **[Flask](https://flask.palletsprojects.com/)** â€“ Lightweight backend API and WebSocket server  
* **[Socket.IO](https://socket.io/)** â€“ Real-time communication between backend and frontend  
* **[React.js](https://reactjs.org/)** â€“ Frontend framework for real-time sentiment dashboard  
* **[Recharts](https://recharts.org/)** â€“ Charting library used in the React dashboard for visualizing sentiment trends  
* **[Docker](https://www.docker.com/)** â€“ Containerized deployment with multi-service orchestration

---

## ðŸš€ Getting Started

### Prerequisites

* Docker & Docker Compose
* YouTube Data API v3 key

### Installation

1. Clone this repository:

```bash
git clone https://github.com/yourusername/your-repo-name.git
cd your-repo-name
```

2. Set up `.env` file with your credentials:

```env
YOUTUBE_API_KEY=your_api_key
VIDEO_ID=your_video_id
KAFKA_TOPIC=your_topic
```
---

## ðŸ“ Project Structure

```
.
â”œâ”€â”€ spark/                      # Spark job for sentiment analysis
â”‚   â”œâ”€â”€ spark_job.py            
â”‚   â”œâ”€â”€ requirements.txt        
â”‚   â””â”€â”€ Dockerfile              
â”‚
â”œâ”€â”€ producer/                   # Kafka producer to collect YouTube live chat
â”‚   â”œâ”€â”€ producer.py             
â”‚   â”œâ”€â”€ requirements.txt        
â”‚   â””â”€â”€ Dockerfile              
â”‚
â”œâ”€â”€ logs/                       # Log analysis scripts
â”‚
â”œâ”€â”€ sentiment-backend/         # Flask backend for REST API and Socket.IO
â”‚   â””â”€â”€ app.py                  
â”‚
â”œâ”€â”€ sentiment-ui/              # React.js frontend for sentiment dashboard
â”‚   â”œâ”€â”€ src/                    
â”‚   â”œâ”€â”€ public/                 
â”‚   â””â”€â”€ index.html              
â”‚
â”œâ”€â”€ docker-compose.yml         # Multi-container orchestration
â”œâ”€â”€ README.md                  
â””â”€â”€ How To Run.txt             # Step-by-step execution guide
```

---

## ðŸ“Œ Usage

1. Kafka producer collects YouTube Live Chat using the Data API v3.
2. Chat messages are streamed to Kafka topics in real-time.
3. Spark Structured Streaming processes messages and performs sentiment analysis.
4. Results are indexed into Elasticsearch.
5. React + Socket.IO dashboard visualizes data in real-time.
6. Kibana dashboard allows for deeper historical analysis.

---

## ðŸ“Š Features

* â±ï¸ Real-time ingestion of YouTube Live Chat messages
* ðŸ”„ Stream processing using Kafka & Spark Structured Streaming
* ðŸ’¬ Sentiment classification via VADER and TextBlob
* ðŸ“ˆ Live dashboard using React + Socket.IO
* ðŸ“Š Historical and advanced analytics via Kibana

![web](https://github.com/user-attachments/assets/07315439-d078-42d7-8e1c-8bd2223742e0)
![Image](https://github.com/user-attachments/assets/a44c717f-5ffc-4f4f-a730-068bee485466)
![Image](https://github.com/user-attachments/assets/e0d0ca6f-3710-4018-9cc4-d2dae758dbb9)
![kibana](https://github.com/user-attachments/assets/45075e59-c8c8-4d1f-b1db-c4159a8594d8)

---

## ðŸ§ª Testing and Performance

The system has been tested with high-traffic YouTube live streams:

| Metric                      | Result                      |
| --------------------------- | --------------------------- |
| Viewer Count (tested)       | 9K, 20K, 100K               |
| Kafka Producer Throughput   | Up to **783 messages/min**  |
| Spark Processing Rate       | Up to **14.75 batches/min** |
| End-to-End Latency          | \~**9 seconds**             |
| VADER Sentiment Accuracy    | **93%**                     |
| TextBlob Sentiment Accuracy | **60%**                     |

---

## ðŸ”® Future Improvements

* ðŸ§  Integrate deep learning models (e.g., BERT, RoBERTa)
* ðŸ” Replace API polling with WebSocket or scraping approach
* âš™ï¸ Migrate from Spark to Flink for lower-latency streaming

---

## âš ï¸ Important Notes

* Requires valid **YouTube Data API v3** key
* Configuration via `.env` file (API key, video ID, Kafka topic, etc.)

---

## ðŸ“ License

This project is licensed under the [MIT License](LICENSE).

![License: Private Use Only](https://img.shields.io/badge/license-private--use--only-lightgrey)

> ðŸ“„ **License**: This project is licensed for **private use only**. Please refer to the [LICENSE](./LICENSE) file for more details.
